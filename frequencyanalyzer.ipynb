{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are assumptions and definitions that limit the scope of the task: <br><br>\n",
    "<b>Word</b>: To simplify, a word is represented by a sequence of one or more characters between „a‟ and „z‟ or between „A‟ and „Z‟). For example “agdfBh”. <br>\n",
    "<b>Letter Case</b>: When counting frequencies, we are interested in the case insensitive frequency (i.e. in the text “The sun shines over the lake”, the library should count 2 occurrences for any of the words “the” or “The” or “tHE” etc). <br>\n",
    "<b>Input Text</b>: The input text contains words separated by various separator characters. Note that the characters from „a‟ and „z‟ and from „A‟ and „Z‟ can only appear within words. <br>\n",
    "<b>Available Memory</b>: There is enough memory to store the whole input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "class FrequencyAnalyzer(object):\n",
    "    \"\"\"Program implements a part of text processing librabry.\n",
    "    \n",
    "    Attributes:\n",
    "        tokens: Tokenized input text.\n",
    "        word: Single word from a text.\n",
    "        n: number of words to be returned.\n",
    "    \"\"\"\n",
    "    hash_map = {}\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "\n",
    "        if tokens is not None:\n",
    "            self.hash_map = {}\n",
    "            for element in tokens:\n",
    "                # Removing comma and full stop\n",
    "                word = element.replace(\",\",\"\")\n",
    "                word = word.replace(\".\",\"\")\n",
    "                \n",
    "                if word in self.hash_map:\n",
    "                    self.hash_map[word] = self.hash_map[word] + 1\n",
    "                else:\n",
    "                    self.hash_map[word] = 1\n",
    "\n",
    "    def calculate_highest_frequency (self, tokens):\n",
    "        \"\"\"Return the highest frequency in the text.\"\"\"\n",
    "        return [word for word, count in self.hash_map.items() if count == max(self.hash_map.values())]\n",
    "\n",
    "    def calculate_frequency_for_word (self, tokens, word):\n",
    "        \"\"\"Return the frequency of the specified word.\"\"\"\n",
    "        if word in tokens:\n",
    "            return self.hash_map[word]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def calculate_most_frequent_n_words (self, tokens, n):\n",
    "        sorted_list = dict(sorted(self.hash_map.items(), key=lambda word: word[0]))\n",
    "        return dict(sorted(sorted_list.items(), key=lambda word: word[1], reverse=True)[:n])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Give choice to user\n",
    "    choice = int(input(\"1. Enter one for entering a text string.\\n 2. Enter one for entering a text file. \"))\n",
    "    \n",
    "    if choice == 1:\n",
    "        textFile = input(\"Enter text. \")\n",
    "    elif choice == 2:\n",
    "        # Reading the input text file\n",
    "        while True:\n",
    "            try:\n",
    "                # Input from user\n",
    "                file_path = input(\"Enter filepath of text file: \")\n",
    "                with open(file_path) as infile:\n",
    "                    file = open(file_path, 'r')\n",
    "                    textFile = file.read()\n",
    "                    break\n",
    "            except IOError:\n",
    "                print(\"Path of the file is Invalid! Note that this is a relative path.\")\n",
    "    else:\n",
    "        print(\"Not a valid option.\")\n",
    "            \n",
    "    \n",
    "    def tokenize():\n",
    "        if textFile is not None:\n",
    "            words = textFile.lower().split()\n",
    "            return words\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    # Tokenize the text file    \n",
    "    words = tokenize()\n",
    "    \n",
    "    freq_analyser = FrequencyAnalyzer(words)\n",
    "    \n",
    "    # Ouputing the highest frequency word\n",
    "    max_occurance = freq_analyser.calculate_highest_frequency(words)\n",
    "    print(\"Highest frequency Word(s): \"+ str(max_occurance))\n",
    "    \n",
    "    # Ouputing frequency for a given word\n",
    "    word = input(\"Enter a of word from text: \")\n",
    "    freq_count = freq_analyser.calculate_frequency_for_word(words, word)\n",
    "    print(\"Frequency of \"+ word + \" is \" + str(freq_count))\n",
    "    \n",
    "    # Ouputing top n frequency words\n",
    "    while True:\n",
    "        try:\n",
    "            n = int(input(\"Enter value of n: \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"That's not a valid option!\")\n",
    "        \n",
    "    top_words = freq_analyser.calculate_most_frequent_n_words(words, n)\n",
    "    print(\"Top \"+ str(n) + \" words are: \")\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Enter one for entering a text string.\n",
      " 2. Enter one for entering a text file. 2\n",
      "Enter filepath of text file: s.txt\n",
      "Highest frequency Word(s): ['and']\n",
      "Enter a of word from text: fast\n",
      "Frequency of fast is 4\n",
      "Enter value of n: 6\n",
      "Top 6 words are: \n",
      "{'and': 490, 'the': 431, 'to': 408, 'my': 390, 'of': 369, 'i': 339}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Testcases</b> using unittest\n",
    "Prerequisite: install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 0 tests in 0.000s\n",
      "\n",
      "OK\n",
      "test_calculate_frequency_for_word (__main__.TestIWordFrequencyAnalyzer) ... ok\n",
      "test_calculate_highest_frequency (__main__.TestIWordFrequencyAnalyzer) ... ok\n",
      "test_calculate_most_frequent_n_words (__main__.TestIWordFrequencyAnalyzer) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from frequencyanalyzer.ipynb\n",
      "{'the': 3, 'sun': 1, 'shines': 1, 'over': 3, 'lake': 1}\n",
      "{'the': 3, 'sun': 1, 'shines': 1, 'over': 3, 'lake': 1}\n",
      "{'the': 3, 'sun': 1, 'shines': 1, 'over': 3, 'lake': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x27e5af88a20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import import_ipynb\n",
    "from frequencyanalyzer import FrequencyAnalyzer\n",
    "\n",
    "class TestIWordFrequencyAnalyzer(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        words_test = ['the', 'sun', 'shines', 'over', 'the', 'lake', 'the', 'over', 'over']\n",
    "        self.func = FrequencyAnalyzer(words_test)\n",
    "        \n",
    "    def test_calculate_highest_frequency(self):\n",
    "        words_test = {'the', 'sun', 'shines', 'over', 'the', 'lake', 'the', 'over', 'over'}\n",
    "        self.assertEqual(self.func.calculate_highest_frequency(words_test), ['the', 'over'])\n",
    " \n",
    "    def test_calculate_frequency_for_word(self):\n",
    "        words_test = ['the', 'sun', 'shines', 'over', 'the', 'lake', 'the', 'over', 'over']\n",
    "        self.assertEqual(self.func.calculate_frequency_for_word(words_test, \"over\"), 3)\n",
    "    \n",
    "    def test_calculate_most_frequent_n_words(self):\n",
    "        words_test = ['the', 'sun', 'shines', 'over', 'the', 'lake', 'the', 'over', 'over']\n",
    "        self.assertDictEqual(self.func.calculate_most_frequent_n_words(words_test, 4), {'over': 3, 'the': 3, 'lake': 1, 'shines': 1})\n",
    "        \n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
